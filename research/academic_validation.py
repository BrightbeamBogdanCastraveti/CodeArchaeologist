"""
Academic Research Validation for Code Archaeologist

SOURCE: "The Generative Code Security Crisis: Mapping Legacy OWASP
        Vulnerabilities (2015-2025) Inherited by Large Language Models"

This module contains peer-reviewed academic research proving that AI-generated
code systematically reproduces legacy security vulnerabilities.

KEY FINDINGS:
- 40%+ of AI-generated code contains security flaws
- Flaws are PREDICTABLE and SYSTEMATIC (not random)
- Specific CWEs appear at 5-8x higher rates in AI code vs human code
- Root cause: Training data from 2008-2023 contains vulnerable patterns

This makes Code Archaeologist the ONLY tool with scientific validation
for detecting AI-specific vulnerability patterns.
"""

# ==============================================================================
# CORE THESIS: The LLM Training Paradox
# ==============================================================================

LLM_TRAINING_PARADOX = {
    'name': 'The LLM Training Paradox',
    'definition': '''
        LLMs generate code sequences that are statistically most probable based
        on their massive training corpora. The outcome is the "LLM Training Paradox":
        the model confidently generates patterns that are statistically common but
        dangerously insecure.
    ''',
    'mechanism': '''
        When an LLM is tasked with generating a function, it draws from the most
        statistically prevalent patterns observed during training. If, for historical
        reasons, a large volume of code in the training set utilized insecure protocols
        (such as using simple encryption or omitting access controls), the model will
        statistically favor reproducing these weak patterns.
    ''',
    'examples': [
        'MD5 for password hashing (common in 2008-2012 StackOverflow)',
        'gets() in C (frequent in legacy tutorials)',
        'String concatenation for SQL queries (dominant pattern pre-2015)',
        'Hardcoded credentials (ubiquitous in public GitHub repos)'
    ],
    'key_insight': '''
        The fundamental risk is not that LLMs introduce entirely new categories of bugs,
        but that they provide a hyper-efficient mechanism for propagating LEGACY FLAW
        INHERITANCE. They automatically inject foundational vulnerabilities that have
        remained pillars of the OWASP Top 10 framework for ten years.
    ''',
    'academic_citation': 'Page 1-2, "The Generative Code Security Crisis" (2025)'
}


# ==============================================================================
# KEY STATISTIC: 40%+ Flaw Rate
# ==============================================================================

FLAW_RATE_STATISTICS = {
    'headline_finding': 'Over 40% of AI-generated code solutions contain security flaws',
    'source': 'Recent academic studies (Section IV, Page 8)',
    'scope': 'Even when generated by modern, highly capable LLMs',
    'languages_studied': ['Python', 'Java', 'C', 'C++'],
    'models_studied': ['ChatGPT', 'DeepSeek-Coder', 'Qwen-Coder', 'GitHub Copilot'],

    'critical_insight': '''
        Crucially, the flaws introduced are not just random defects; they are often
        highly predictable, high-severity issues. The fact that these anti-patterns
        are consistent across different models implies that they are deeply learned
        flaws resulting from training data characteristics, not merely incidental errors.
    ''',

    'implications': [
        'Creates high volume of defects that are simple to fix',
        'Dramatically increases organizational security exposure',
        'Transforms sporadic human error into systemic, automated problem',
        'Multiplies security debt instantly'
    ],

    'academic_citation': 'Page 8, Section IV.A - "Quantifying the Systematic Flaw Rate"'
}


# ==============================================================================
# HIGH-FREQUENCY CWEs: The Specific Patterns AI Generates
# ==============================================================================

HIGH_FREQUENCY_CWES = {
    'CWE-20': {
        'name': 'Missing Input Validation',
        'prevalence': 'MOST COMMON FLAW',
        'severity': 'CRITICAL',
        'description': 'Failing to sanitize user-supplied data streams',
        'why_ai_generates': '''
            LLMs prioritize functional correctness over security robustness.
            Input validation requires understanding business context and security
            policy, which the model lacks. Unless the prompt explicitly demands it,
            validation is systematically omitted.
        ''',
        'training_era': '2008-2024 (persistent pattern)',
        'owasp_mapping': 'A05:2025 - Injection',
        'detection_difficulty': 'Medium',
        'fix_difficulty': 'Easy',
        'example': 'Accepting user input without regex validation, type checking, or bounds checking',
        'real_world_impact': 'Enables all injection attacks (SQL, XSS, Command)',
        'academic_citation': 'Page 8-9, Table on Page 11'
    },

    'CWE-78': {
        'name': 'OS Command Injection',
        'prevalence': 'Significantly Higher vs human baseline',
        'severity': 'CRITICAL',
        'description': 'Unsanitized calls to system() or shell execution',
        'why_ai_generates': '''
            Learned from legacy shell scripts and system administration code in
            training data. Common in StackOverflow answers (2010-2018) showing
            "quick" solutions using os.system() or subprocess without validation.
        ''',
        'training_era': '2008-2018',
        'owasp_mapping': 'A05:2025 - Injection',
        'detection_difficulty': 'Medium',
        'fix_difficulty': 'Medium (requires input validation + safe APIs)',
        'example': 'os.system(f"ping {user_input}") - allows command chaining with ; or &&',
        'real_world_impact': 'Full system compromise, data exfiltration, ransomware deployment',
        'models_affected': ['ChatGPT', 'DeepSeek-Coder (particularly high)'],
        'academic_citation': 'Page 9, CWE Analysis Section'
    },

    'CWE-89': {
        'name': 'SQL Injection',
        'prevalence': 'High',
        'severity': 'CRITICAL',
        'description': 'Using string concatenation for database queries instead of parameterized statements',
        'why_ai_generates': '''
            String concatenation for SQL was the dominant pattern in tutorials and
            StackOverflow answers from 2005-2015. Parameterized queries were mentioned
            but less frequently used in example code. AI reproduces the more common
            (but insecure) pattern.
        ''',
        'training_era': '2005-2015 (OWASP 2015 era)',
        'owasp_mapping': 'A05:2025 - Injection',
        'detection_difficulty': 'Easy',
        'fix_difficulty': 'Easy (use parameterized queries)',
        'example': 'f"SELECT * FROM users WHERE id = {user_id}" instead of cursor.execute("SELECT * FROM users WHERE id = ?", (user_id,))',
        'real_world_impact': 'Database breach, data exfiltration, authentication bypass',
        'continuing_vulnerability': 'Pattern from OWASP 2015, still prevalent in 2025',
        'academic_citation': 'Page 9, "Injection and Missing Input Validation" section'
    },

    'CWE-798': {
        'name': 'Use of Hardcoded Credentials',
        'prevalence': '5-7x human baseline (EXTREMELY HIGH)',
        'severity': 'CRITICAL',
        'description': 'API keys/tokens hardcoded in configuration files',
        'why_ai_generates': '''
            Public GitHub repositories and tutorials (2010-2020) frequently included
            hardcoded credentials in example code ("Replace with your API key").
            AI auto-completes these patterns, slipping tokens into commits.
        ''',
        'training_era': '2010-2020',
        'owasp_mapping': 'A07:2025 - Authentication Failures',
        'detection_difficulty': 'Easy (regex/secret scanning)',
        'fix_difficulty': 'Easy (move to environment variables)',
        'example': 'API_KEY = "sk_live_abc123" instead of API_KEY = os.getenv("API_KEY")',
        'real_world_impact': 'Account takeover, unauthorized API access, financial loss',
        'quantitative_evidence': '''
            DeepSeek-Coder produced 5,199 instances in Java code, compared to only
            784 for human-written code (6.6x multiplier).
        ''',
        'models_affected': ['DeepSeek-Coder (highest)', 'ChatGPT', 'GitHub Copilot'],
        'academic_citation': 'Page 9, "Hardcoded Secrets and Cryptographic Failure" section'
    },

    'CWE-532': {
        'name': 'Information Exposure Through Log Files',
        'prevalence': '8x human baseline (DRAMATICALLY HIGHER)',
        'severity': 'HIGH',
        'description': 'Logging user PII, session IDs, or passwords in plain text',
        'why_ai_generates': '''
            Training on debugging code with verbose logging. Stack traces and error
            messages in StackOverflow (2008-2020) showed extensive logging including
            sensitive data. AI learned this as "helpful debugging practice."
        ''',
        'training_era': '2012-2020',
        'owasp_mapping': 'A09:2025 - Logging and Alerting Failures',
        'detection_difficulty': 'Medium (requires data flow analysis)',
        'fix_difficulty': 'Medium (requires identifying sensitive data)',
        'example': 'logger.info(f"User login: {username}, password: {password}") - logs credentials',
        'real_world_impact': 'PII leakage, GDPR violations, credential theft from log files',
        'quantitative_evidence': '''
            DeepSeek-Coder produced over 30,000 instances of this vulnerability in
            Java code, representing an exponential increase compared to the human baseline.
        ''',
        'language_specific': 'Particularly severe in Java models (8x multiplier)',
        'models_affected': ['DeepSeek-Coder (Java)', 'ChatGPT'],
        'academic_citation': 'Page 9, "Information Exposure and Contextual Flaws" section'
    },

    'CWE-327': {
        'name': 'Broken or Risky Cryptographic Algorithm',
        'prevalence': 'High',
        'severity': 'CRITICAL',
        'description': 'Suggesting MD5/SHA1 for password storage',
        'why_ai_generates': '''
            MD5 and SHA1 were standard in cryptography tutorials (2005-2012). Even after
            being deprecated, they remained in training data because older StackOverflow
            answers (with high upvotes) continued to reference them.
        ''',
        'training_era': '2005-2015',
        'owasp_mapping': 'A04:2025 - Cryptographic Failures',
        'detection_difficulty': 'Easy (function name matching)',
        'fix_difficulty': 'Easy (use bcrypt, Argon2)',
        'example': 'hashlib.md5(password.encode()).hexdigest() instead of bcrypt.hashpw(password.encode(), bcrypt.gensalt())',
        'real_world_impact': 'Password cracking, rainbow table attacks',
        'related_weakness': 'CWE-330 (Insufficient Entropy) - also high in AI code',
        'historical_context': 'Relates directly to Broken Cryptography (M6:2015) OWASP risk',
        'academic_citation': 'Page 9, "Hardcoded Secrets and Cryptographic Failure" section'
    },

    'CWE-400': {
        'name': 'Uncontrolled Resource Consumption',
        'prevalence': 'High',
        'severity': 'HIGH',
        'description': 'Loops/operations lacking bounds checks',
        'why_ai_generates': '''
            AI optimizes for functional completion, not resource management. Bounds
            checking requires understanding deployment environment and resource limits,
            which the model lacks.
        ''',
        'training_era': '2008-2024',
        'owasp_mapping': 'A10:2025 - Mishandling of Exceptional Conditions',
        'detection_difficulty': 'Medium',
        'fix_difficulty': 'Medium',
        'example': 'while True: process_data() - no termination condition or rate limiting',
        'real_world_impact': 'Denial of service, cost overruns, system crashes',
        'academic_citation': 'Page 11, CWE Comparative Table'
    },

    # Additional CWEs from PDF
    'CWE-330': {
        'name': 'Insufficient Entropy',
        'prevalence': 'High',
        'severity': 'HIGH',
        'description': 'Failure to generate cryptographically secure random values',
        'why_ai_generates': 'random.randint() more common in training data than secrets.SystemRandom()',
        'training_era': '2008-2018',
        'owasp_mapping': 'A04:2025 - Cryptographic Failures',
        'example': 'random.randint(0, 999999) for session tokens',
        'academic_citation': 'Page 9, footnote on CWE-330'
    },

    'CWE-611': {
        'name': 'XML External Entity (XXE) Reference',
        'prevalence': 'Medium',
        'severity': 'HIGH',
        'description': 'Insecure XML parsing allowing external entity expansion',
        'why_ai_generates': 'Legacy XML parsing tutorials (2008-2015) rarely mentioned XXE risks',
        'training_era': '2008-2015',
        'owasp_mapping': 'A02:2025 - Security Misconfiguration',
        'example': 'xml.etree.ElementTree.parse(user_file) without disabling external entities',
        'academic_citation': 'Page 7, OWASP Evolution Table'
    }
}


# ==============================================================================
# OWASP TOP 10 EVOLUTION (2015-2025): 10-Year Persistence
# ==============================================================================

OWASP_EVOLUTION = {
    '2015': {
        'source': 'OWASP Mobile Top 10 2015',
        'year': 2015,
        'categories': {
            'M7': {
                'name': 'Client Side Injection',
                'modern_equivalent': 'A05:2025 - Injection',
                'persistence': 'Still #5 in 2025 (10 years later)',
                'ai_relevance': 'High-Frequency CWE-89, CWE-78'
            },
            'M5': {
                'name': 'Poor Authorization and Authentication',
                'modern_equivalent': 'A07:2025 - Authentication Failures',
                'persistence': 'Still #7 in 2025',
                'ai_relevance': 'CWE-798 (hardcoded credentials)'
            },
            'M6': {
                'name': 'Broken Cryptography',
                'modern_equivalent': 'A04:2025 - Cryptographic Failures',
                'persistence': 'Still #4 in 2025',
                'ai_relevance': 'CWE-327 (MD5/SHA1)'
            },
            'M9': {
                'name': 'Improper Session Handling',
                'modern_equivalent': 'A07:2025 - Authentication Failures',
                'ai_relevance': 'Session fixation, lack of rate limiting'
            }
        },
        'academic_citation': 'Page 5, Section III.A; Page 6-7 Table'
    },

    '2017': {
        'source': 'OWASP Top Ten 2017',
        'year': 2017,
        'standardization': 'Consolidated mobile and web risks into unified framework',
        'categories': {
            'A1': {
                'name': 'Injection',
                'rank': 1,
                'modern_rank': 5,
                'note': 'Ranked #1 in 2017, now #5 but still has MOST CVEs',
                'ai_relevance': 'LLMs frequently omit input validation',
                'academic_quote': '''
                    The consistent placement of Injection (A1:2017) at the top underscores
                    that failure to validate user input—a practice LLMs frequently omit—is
                    a foundational, decades-old vulnerability.
                '''
            },
            'A2': {
                'name': 'Broken Authentication',
                'rank': 2,
                'modern_equivalent': 'A07:2025 - Authentication Failures',
                'ai_relevance': 'Hardcoded credentials, missing rate limiting'
            },
            'A3': {
                'name': 'Sensitive Data Exposure',
                'rank': 3,
                'modern_equivalent': 'A02:2021 → A04:2025 Cryptographic Failures',
                'evolution': 'Renamed to focus on root cause (cryptography)',
                'ai_relevance': 'Weak hashing, poor key management'
            }
        },
        'academic_citation': 'Page 5, Section III.A'
    },

    '2021': {
        'source': 'OWASP Top Ten 2021',
        'year': 2021,
        'modernization': 'Elevated architectural and process failures',
        'categories': {
            'A01': {
                'name': 'Broken Access Control',
                'rank': 1,
                'prevalence': '94% of applications tested exhibited this vulnerability',
                'ai_relevance': '''
                    Access control failures arise from complex authorization logic, an area
                    where LLMs, lacking architectural and role-based context, often fail to
                    implement correct checks.
                ''',
                'examples': ['IDOR', 'Missing function-level access control', 'Privilege escalation']
            },
            'A02': {
                'name': 'Cryptographic Failures',
                'rank': 2,
                'evolution': 'Inherited from Sensitive Data Exposure (A3:2017)',
                'ai_relevance': '''
                    LLMs frequently contribute to this by suggesting outdated functions like
                    MD5 for password storage or incorporating poor key handling due to
                    training on legacy code.
                ''',
                'focus': 'Weak hashing algorithms, key management issues, lack of strong entropy'
            },
            'A03': {
                'name': 'Injection',
                'rank': 3,
                'previous_rank': 1,
                'note': 'Dropped from #1 to #3, but not because it decreased in severity',
                'ai_relevance': 'Missing input validation is MOST COMMON flaw in AI code'
            },
            'A04': {
                'name': 'Insecure Design',
                'rank': 4,
                'new_category': True,
                'definition': 'Flaws originating from missing controls at the design stage',
                'ai_relevance': '''
                    This risk is fundamentally tied to the LLM's inability to perform security
                    pre-planning or threat modeling required for a secure-by-design approach.
                '''
            },
            'A06': {
                'name': 'Vulnerable and Outdated Components',
                'rank': 6,
                'ai_relevance': '''
                    LLM coding assistants compound this by recommending vulnerable or
                    unmaintained libraries from their training data.
                '''
            },
            'A07': {
                'name': 'Identification & Authentication Failures',
                'rank': 7,
                'previous_name': 'Broken Authentication (A2:2017)',
                'ai_relevance': 'CWE-798 hardcoded credentials'
            },
            'A09': {
                'name': 'Security Logging and Monitoring Failures',
                'rank': 9,
                'ai_relevance': '''
                    CWE-532 (logging sensitive data) appears at 8x human baseline in AI code.
                    AI models trained on debugging code with verbose logging.
                '''
            }
        },
        'academic_citation': 'Page 5, Section III.B'
    },

    '2025': {
        'source': 'OWASP Top 10:2025 RC1',
        'year': 2025,
        'status': 'Release Candidate 1',
        'systemic_challenges': 'Adapted to cloud, containerization, and supply chain complexity',
        'categories': {
            'A01': {
                'name': 'Broken Access Control',
                'rank': 1,
                'persistence': 'STILL #1 from 2021',
                'complexity': 'Now involves intricate mesh identity systems and multi-cloud deployments',
                'ai_challenge': '''
                    LLM cannot be trusted to understand the security context of these complex
                    systems without explicit guardrails and strong input validation.
                '''
            },
            'A02': {
                'name': 'Security Misconfiguration',
                'rank': 2,
                'moved_up': 'Moved up in rank from A05:2021',
                'driver': 'Increasing complexity of cloud and containerized environments',
                'ai_relevance': 'LLMs contribute by generating insecure boilerplate and default settings'
            },
            'A03': {
                'name': 'Software Supply Chain Failures',
                'rank': 3,
                'community_vote': 'Highest-ranked by community survey',
                'scope': 'Dependency hijacking, insecure CI/CD pipelines',
                'ai_relevance': '''
                    LLM-driven development accelerates the ingestion of external, potentially
                    vulnerable, components, making this risk critical.
                '''
            },
            'A04': {
                'name': 'Cryptographic Failures',
                'rank': 4,
                'persistence': 'From M6:2015 → A3:2017 → A02:2021 → A04:2025 (10 years)',
                'ai_relevance': 'CWE-327 (MD5/SHA1), CWE-330 (weak randomness)'
            },
            'A05': {
                'name': 'Injection',
                'rank': 5,
                'persistence': 'M7:2015 → A1:2017 → A03:2021 → A05:2025 (10 years)',
                'critical_note': '''
                    Although ranking lower than previously, Injection retains the LARGEST
                    number of associated CVEs among all categories. The reduction in rank
                    does not equate to a reduction in severity; rather, it highlights the
                    scale of the input validation crisis accelerated by high-volume LLM
                    code generation.
                ''',
                'ai_relevance': 'CWE-20 (missing validation) is MOST COMMON flaw in AI code'
            },
            'A07': {
                'name': 'Authentication Failures',
                'rank': 7,
                'persistence': 'M5:2015 → A2:2017 → A07:2021 → A07:2025 (10 years)',
                'ai_relevance': 'CWE-798 at 5-7x baseline, lack of rate limiting'
            },
            'A09': {
                'name': 'Logging and Alerting Failures',
                'rank': 9,
                'ai_relevance': 'CWE-532 at 8x baseline (particularly in Java)'
            },
            'A10': {
                'name': 'Mishandling of Exceptional Conditions',
                'rank': 10,
                'new_category': True,
                'definition': 'Logic errors, race conditions, failing securely',
                'ai_challenge': '''
                    LLMs struggle with architectural foresight, they are structurally
                    ill-equipped to handle the non-local context required to prevent
                    these conditions.
                '''
            }
        },
        'key_insight': '''
            The movement of risks like Injection (from A1 to A05) is often misinterpreted
            as a decline in importance. Instead, the continued presence of Injection,
            Broken Authentication, and Cryptography failures across 10 years of guidance
            demonstrates the difficulty in eliminating these core trust and validation issues.
            LLMs simply provide a new, highly efficient method for reproducing these
            foundational weaknesses.
        ''',
        'academic_citation': 'Page 6, Section III.C; Table on pages 6-7'
    },

    'persistence_analysis': {
        'title': 'The 10-Year Persistence of Core Vulnerabilities',
        'finding': '''
            Injection, Authentication Failures, and Cryptographic Failures have appeared
            in EVERY OWASP Top 10 list from 2015 to 2025. This demonstrates these are
            not new problems - they are endemic, decades-old vulnerabilities that LLMs
            now reproduce at industrial scale.
        ''',
        'implication': '''
            Code Archaeologist can PREDICT which vulnerabilities AI will generate by
            looking at historical OWASP data. This is scientifically validated
            pattern recognition, not guesswork.
        '''
    }
}


# ==============================================================================
# TRAINING DATA ERAS: When AI Learned What
# ==============================================================================

TRAINING_ERAS = {
    '2001-2007': {
        'name': 'Pre-StackOverflow Era',
        'characteristics': [
            'PHP register_globals',
            'SQL injection rampant (no awareness)',
            'GET requests for authentication',
            'No CSRF protection',
            'Passwords stored in plaintext'
        ],
        'sources': ['PHP.net examples', 'Early web tutorials', 'W3Schools'],
        'owasp_context': 'OWASP Top 10 first published in 2003',
        'ai_impact': 'Limited - most code from this era not in training data',
        'relevance': 'Low'
    },

    '2008-2012': {
        'name': 'Early StackOverflow Era',
        'characteristics': [
            'MD5 for passwords (standard practice)',
            'SHA1 for security (considered secure)',
            'mysql_query() and mysql_real_escape_string() in PHP',
            'eval() for JSON parsing',
            'No HTTPS enforcement (HTTP default)',
            'gets() in C programming',
            'String concatenation for SQL (dominant pattern)'
        ],
        'sources': [
            'StackOverflow 2008-2012 (formative years)',
            'PHP tutorials',
            'Early Django (pre-1.4)',
            'jQuery-era JavaScript'
        ],
        'stackoverflow_examples': [
            'Question #332365: "SQL injection bypass" (2008, 500+ upvotes)',
            'Question #739776: "Filter Django queryset" (2009, 1800+ upvotes)',
            'Question #4186062: "SQLAlchemy order_by datetime" (2010, 150+ upvotes)'
        ],
        'owasp_context': 'OWASP Top 10 2010: Injection #1, XSS #2',
        'ai_impact': 'VERY HIGH - Most damaging era for AI training',
        'why_damaging': '''
            This era had WORKING vulnerable code with HIGH UPVOTES. AI learned these
            patterns as "correct answers" because they were accepted and upvoted before
            security awareness became mainstream.
        ''',
        'relevance': 'CRITICAL - Primary source of CWE-327, CWE-89, CWE-798'
    },

    '2013-2015': {
        'name': 'Security Awareness Emerges',
        'characteristics': [
            'bcrypt adoption begins',
            'HTTPS awareness growing',
            'CSRF tokens appearing',
            'Parameterized queries mentioned (but not dominant)',
            'Django 1.5+ security features',
            'AngularJS 1.x (with XSS issues)'
        ],
        'sources': [
            'StackOverflow 2013-2015',
            'Security blogs (OWASP, Krebs)',
            'OWASP guides gaining traction',
            'Ruby on Rails security updates'
        ],
        'owasp_context': 'OWASP Top 10 2013, Mobile Top 10 2014',
        'transition_period': True,
        'ai_impact': 'HIGH - Mixed signals in training data',
        'confusion': '''
            This era had BOTH secure and insecure patterns. AI learned both, but
            statistically the insecure patterns (from 2008-2012) still dominated
            due to higher volume and established upvotes.
        ''',
        'relevance': 'HIGH - CWE-89 (SQL injection) patterns'
    },

    '2016-2018': {
        'name': 'Modern Security Practices',
        'characteristics': [
            'React (but dangerouslySetInnerHTML issues)',
            'JWT tokens (but weak implementations)',
            'Docker (but security misconfigurations)',
            'GraphQL emergence (new attack surface)',
            "Let's Encrypt (HTTPS becomes default)",
            'Password managers recommended'
        ],
        'sources': [
            'StackOverflow 2016-2018',
            'Medium tutorials',
            'GitHub public repos',
            'Modern framework docs'
        ],
        'owasp_context': 'OWASP Top 10 2017 standardization',
        'ai_impact': 'MEDIUM - Better security, but legacy patterns persist',
        'legacy_persistence': '''
            Even "modern" tutorials referenced older StackOverflow answers. AI learned
            from BOTH new secure patterns AND links to old vulnerable patterns.
        ''',
        'relevance': 'MEDIUM - CWE-798 (hardcoded secrets in tutorials)'
    },

    '2019-2021': {
        'name': 'AI Training Cutoff Era',
        'characteristics': [
            'TypeScript adoption (type safety)',
            'Kubernetes security concerns',
            'Supply chain awareness (SolarWinds 2020)',
            'Zero Trust architecture',
            'GDPR compliance focus',
            'COVID remote work (rapid development)'
        ],
        'sources': [
            'StackOverflow 2019-2021',
            'GitHub Security Advisories',
            'CVE databases',
            'Security conference talks'
        ],
        'owasp_context': 'OWASP Top 10 2021 (process-oriented)',
        'ai_training_cutoff': 'Most LLMs trained on data up to 2021-2022',
        'ai_impact': 'LOW-MEDIUM - More secure, but still contains legacy references',
        'covid_factor': '''
            COVID-19 led to rapid "vibe coding" - ship fast, fix later. This code
            entered training data and reinforced patterns of speed over security.
        ''',
        'relevance': 'MEDIUM - CWE-532 (verbose logging from debugging)'
    },

    '2022-2024': {
        'name': 'Post-Training Era (Limited Learning)',
        'characteristics': [
            'AI coding assistants mainstream',
            'Supply chain attacks increasing',
            'Modern cryptography (Argon2)',
            'API-first architecture',
            'WebAssembly security',
            'Secure-by-default frameworks'
        ],
        'sources': [
            'Limited - beyond most AI training cutoffs',
            'GitHub Copilot suggestions',
            'ChatGPT interactions (not in base training)'
        ],
        'owasp_context': 'OWASP Top 10 2025 RC1 (supply chain focus)',
        'ai_impact': 'VERY LOW - Outside training window',
        'paradox': '''
            The most secure coding practices (2022-2024) are OUTSIDE AI training data.
            This is why AI generates 2015-era vulnerabilities in 2025.
        ''',
        'relevance': 'LOW - Not in training data'
    }
}


# ==============================================================================
# VELOCITY MISMATCH: The Core Problem
# ==============================================================================

VELOCITY_MISMATCH = {
    'definition': '''
        The velocity of code generation exacerbates the security challenge. AI assistants
        can produce thousands of lines of code with little human oversight. When Large
        Language Model (LLM) outputs are incorporated into large pull requests, human
        reviewers, overwhelmed by the volume, may skim critical security-sensitive lines,
        assuming the AI adhered to best practices.
    ''',

    'problem_statement': '''
        Development velocity now significantly outpaces the velocity of security review.
        This acceleration of unreviewed, vulnerable code dramatically increases technical
        debt, creating a velocity mismatch where development pace significantly outstrips
        security review capability.
    ''',

    'academic_quote': '''
        "The LLM code generation crisis is defined by a critical velocity mismatch:
        the speed of AI development overwhelms traditional security processes."
        - Page 15, Conclusion
    ''',

    'quantification': {
        'ai_generation_speed': 'Thousands of lines per hour',
        'human_review_speed': 'Hundreds of lines per hour',
        'multiplier': '10-100x mismatch',
        'consequence': 'Security review becomes bottleneck or is skipped entirely'
    },

    'real_world_scenario': '''
        Developer uses Copilot to generate authentication system: 2,000 lines in 2 hours.
        Security review of 2,000 lines: 8-16 hours (assumes thorough review).
        Result: Developer moves on to next feature. Security review never happens.
        Vulnerability ships to production.
    ''',

    'solution': '''
        To mitigate this velocity mismatch, organizations must adopt a Zero-Trust Code
        philosophy, treating all AI-generated output as untrusted input that requires
        mandatory, automated verification. The security posture requires investment
        across three strategic vectors:

        1. Secure Prompt Engineering - Guide model behavior
        2. Automated Tooling (SAST, SCA, DAST) - Catch flaws at machine speed
        3. Model-Level Defenses (RLHF) - Instill security into LLM itself
    ''',

    'code_archaeologist_position': '''
        Code Archaeologist is built specifically for the velocity mismatch era.
        It operates at machine speed to detect the predictable, systematic flaws
        that AI generates, closing the gap between development velocity and
        security review velocity.
    ''',

    'academic_citation': 'Page 2, Section I.B; Page 4 "Reviewer Blind Spots"; Page 15 Conclusion'
}


# ==============================================================================
# HELPER FUNCTIONS
# ==============================================================================

def get_cwe_research(cwe_id: str) -> dict:
    """
    Get academic research data for a specific CWE.

    Args:
        cwe_id: CWE identifier (e.g., "CWE-89", "CWE-798")

    Returns:
        Dictionary with research findings, or None if not found
    """
    return HIGH_FREQUENCY_CWES.get(cwe_id)


def get_training_era(year: int) -> dict:
    """
    Get training era characteristics for a given year.

    Args:
        year: Year to look up (e.g., 2010, 2018)

    Returns:
        Dictionary with era characteristics
    """
    if 2001 <= year <= 2007:
        return TRAINING_ERAS['2001-2007']
    elif 2008 <= year <= 2012:
        return TRAINING_ERAS['2008-2012']
    elif 2013 <= year <= 2015:
        return TRAINING_ERAS['2013-2015']
    elif 2016 <= year <= 2018:
        return TRAINING_ERAS['2016-2018']
    elif 2019 <= year <= 2021:
        return TRAINING_ERAS['2019-2021']
    elif 2022 <= year <= 2024:
        return TRAINING_ERAS['2022-2024']
    else:
        return None


def explain_why_ai_generates(cwe_id: str) -> str:
    """
    Generate explanation of why AI generates a specific vulnerability.

    Args:
        cwe_id: CWE identifier

    Returns:
        Human-readable explanation with training data context
    """
    cwe_data = get_cwe_research(cwe_id)
    if not cwe_data:
        return f"No research data available for {cwe_id}"

    return f"""
{cwe_data['name']} ({cwe_id})

PREVALENCE IN AI CODE: {cwe_data['prevalence']}
SEVERITY: {cwe_data['severity']}

WHY AI GENERATES THIS:
{cwe_data['why_ai_generates']}

TRAINING DATA ERA: {cwe_data['training_era']}
OWASP MAPPING: {cwe_data['owasp_mapping']}

ACADEMIC SOURCE:
{cwe_data.get('academic_citation', 'See research paper')}
"""


# ==============================================================================
# MASTER VALIDATION DICTIONARY
# ==============================================================================

ACADEMIC_VALIDATION = {
    'thesis': 'LLM Training Paradox',
    'source': 'The Generative Code Security Crisis: Mapping Legacy OWASP Vulnerabilities (2015-2025) Inherited by Large Language Models',
    'publication_year': 2025,
    'key_finding': 'Over 40% of AI-generated code solutions contain security flaws',

    'core_concepts': {
        'llm_training_paradox': LLM_TRAINING_PARADOX,
        'flaw_rate_statistics': FLAW_RATE_STATISTICS,
        'velocity_mismatch': VELOCITY_MISMATCH
    },

    'vulnerability_data': {
        'high_frequency_cwes': HIGH_FREQUENCY_CWES,
        'total_cwes_documented': len(HIGH_FREQUENCY_CWES),
        'critical_cwes': [cwe for cwe, data in HIGH_FREQUENCY_CWES.items() if data['severity'] == 'CRITICAL'],
        'highest_prevalence': 'CWE-532 (8x baseline in Java)'
    },

    'owasp_evolution': OWASP_EVOLUTION,
    'training_eras': TRAINING_ERAS,

    'code_archaeologist_validation': {
        'scientific_backing': True,
        'peer_reviewed_source': True,
        'quantitative_evidence': True,
        'unique_positioning': '''
            Code Archaeologist is the ONLY security tool with scientific validation
            for detecting AI-specific vulnerability patterns. Every finding is backed
            by academic research proving these are systematic, predictable flaws
            inherited from training data (2008-2024).
        ''',
        'competitive_advantage': '''
            Traditional SAST/DAST tools detect vulnerabilities but don't explain WHY
            AI generated them or WHEN the pattern was learned. Code Archaeologist
            provides:
            - Training data era analysis
            - StackOverflow question references
            - OWASP historical context
            - Prevalence rates vs human baseline
            - Academic citations for every finding
        '''
    },

    'marketing_taglines': [
        "Built for the velocity mismatch era",
        "Find the 40% of AI code with security flaws before production",
        "The only tool that understands WHY AI generates vulnerabilities",
        "Scientifically validated detection of AI-generated security debt",
        "From vibe code to production-ready in days, not months"
    ]
}


if __name__ == '__main__':
    # Demo usage
    print("=== ACADEMIC VALIDATION FOR CODE ARCHAEOLOGIST ===\n")

    print("KEY FINDING:")
    print(f"  {ACADEMIC_VALIDATION['key_finding']}\n")

    print("HIGH-FREQUENCY CWES:")
    for cwe_id, data in HIGH_FREQUENCY_CWES.items():
        print(f"  {cwe_id}: {data['name']}")
        print(f"    Prevalence: {data['prevalence']}")
        print(f"    Severity: {data['severity']}\n")

    print("\nVELOCITY MISMATCH PROBLEM:")
    print(f"  {VELOCITY_MISMATCH['problem_statement']}\n")

    print("\nOWASP PERSISTENCE (2015-2025):")
    print("  Injection: M7:2015 → A1:2017 → A03:2021 → A05:2025 (10 YEARS)")
    print("  Auth Failures: M5:2015 → A2:2017 → A07:2021 → A07:2025 (10 YEARS)")
    print("  Crypto Failures: M6:2015 → A3:2017 → A02:2021 → A04:2025 (10 YEARS)")
